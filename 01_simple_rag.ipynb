{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f77dfa",
   "metadata": {},
   "source": [
    "# RAG TAG: Simple RAG Pipeline with Ollama and Mistral\n",
    "    \n",
    "This notebook demonstrates a minimal implementation of a Retrieval-Augmented Generation (RAG) workflow using:\n",
    "\n",
    "- PyMuPDF (fitz) for text extraction,\n",
    "- SentenceTransformers for embeddings,\n",
    "- FAISS for vector search,\n",
    "- Ollama with Mistral for text generation,\n",
    "\n",
    "No LangChain or other high-level frameworks are used.\n",
    "# Introduction to Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\n",
    "\n",
    "In a Simple RAG setup, we follow these steps:\n",
    "\n",
    "1. **Data Ingestion**: Load and preprocess the text data.\n",
    "2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n",
    "3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieved text.\n",
    "\n",
    "This notebook implements a Simple RAG approach, evaluates the modelâ€™s response, and explores various improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44ae7d",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe26496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import numpy as np\n",
    "import requests  # For Ollama API\n",
    "from sentence_transformers import SentenceTransformer  # For local embeddings\n",
    "import faiss  # For vector search\n",
    "import json  # For loading validation data\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf34248",
   "metadata": {},
   "source": [
    "# 1. Extract text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a281c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589b9f0",
   "metadata": {},
   "source": [
    "# 2. Chunk text into segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1045327",
   "metadata": {},
   "source": [
    "# 3. Create embeddings using local Sentence Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts):\n",
    "    \"\"\"Generate embeddings for a list of text chunks.\"\"\"\n",
    "    # Initialize the model (only happens once)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    normalized_embeddings = np.array([emb / np.linalg.norm(emb) for emb in embeddings]).astype('float32')\n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e810d03",
   "metadata": {},
   "source": [
    "# 4. Create FAISS index for vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717faa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(embeddings):\n",
    "    \"\"\"Create a FAISS index for fast vector search.\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "    index.add(embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d1ec0",
   "metadata": {},
   "source": [
    "# 5. Search for relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vectors(index, query_embedding, texts, k=3):\n",
    "    \"\"\"Find the k most relevant text chunks for a query embedding.\"\"\"\n",
    "    # Normalize query embedding\n",
    "    query_embedding = np.array([query_embedding / np.linalg.norm(query_embedding)]).astype('float32')\n",
    "    \n",
    "    # Search for similar vectors\n",
    "    similarities, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Return the actual text chunks and their similarity scores\n",
    "    return [texts[i] for i in indices[0]], similarities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ec21f",
   "metadata": {},
   "source": [
    "# 6. Query Ollama with Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aee120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama_mistral(prompt):\n",
    "    \"\"\"Send a prompt to Ollama's Mistral model and get a response.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"mistral\",\n",
    "                \"prompt\": prompt\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        return response.json()[\"response\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Ollama: {e}\")\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14b260",
   "metadata": {},
   "source": [
    "# 7. Generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context):\n",
    "    \"\"\"Generate a response to the query based on the provided context.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please answer the following question based only on the provided context information.\n",
    "    If the answer cannot be determined from the context, respond with \"I don't have enough information to answer that.\"\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return query_ollama_mistral(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573de682",
   "metadata": {},
   "source": [
    "# 8. Complete RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, file_path):\n",
    "    \"\"\"Run the complete RAG pipeline from document to answer.\"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"Error: File {file_path} not found\", [], []\n",
    "    \n",
    "    # Extract text based on file type\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    \n",
    "    # Chunk the text\n",
    "    chunks = chunk_text(text)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for chunks\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create vector index\n",
    "    index = create_vector_index(chunk_embeddings)\n",
    "    \n",
    "    # Create query embedding\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Search for relevant chunks\n",
    "    relevant_chunks, similarities = search_vectors(index, query_embedding, chunks)\n",
    "    \n",
    "    # Join retrieved contexts\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    \n",
    "    # Generate response using Mistral\n",
    "    start_time = time.time()\n",
    "    response = generate_response(query, context)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Response generated in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return response, relevant_chunks, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c4ca6",
   "metadata": {},
   "source": [
    "# 9. Example usage\n",
    "\n",
    "# Make sure Ollama is running and Mistral model is downloaded (ollama pull mistral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5251a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure Ollama is running and Mistral model is downloaded (ollama pull mistral)\n",
    "file_path = \"data/quantum.txt\"  # Update with your actual file path\n",
    "query = \"What are the stages in the path to practical quantum computing?\"\n",
    "\n",
    "print(\"Running RAG pipeline with Ollama + Mistral...\\n\")\n",
    "response, chunks, scores = rag_pipeline(query, file_path)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nResponse from Mistral:\\n{response}\")\n",
    "print(\"\\nTop retrieved chunks:\")\n",
    "for i, (chunk, score) in enumerate(zip(chunks, scores)):\n",
    "    print(f\"\\nChunk {i+1} (similarity score: {score:.4f}):\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d70df",
   "metadata": {},
   "source": [
    "# 10. Evaluation with validation data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_validation_data(val_file_path, document_path):\n",
    "    \"\"\"Evaluate the RAG pipeline using validation data.\"\"\"\n",
    "    try:\n",
    "        with open(val_file_path, 'r') as f:\n",
    "            val_data = json.load(f)\n",
    "        \n",
    "        results = []\n",
    "        for item in val_data:\n",
    "            query = item[\"question\"]\n",
    "            print(f\"Testing: {query}\")\n",
    "            \n",
    "            response, chunks, scores = rag_pipeline(query, document_path)\n",
    "            \n",
    "            results.append({\n",
    "                \"question\": query,\n",
    "                \"generated_answer\": response,\n",
    "                \"ideal_answer\": item[\"ideal_answer\"],\n",
    "                \"has_answer\": item[\"has_answer\"],\n",
    "                \"top_chunk\": chunks[0][:200] + \"...\" if chunks and len(chunks[0]) > 200 else chunks[0] if chunks else \"\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Uncomment to run evaluation\n",
    "# val_results = evaluate_with_validation_data(\"data/val.json\", file_path)\n",
    "# print(\"\\n--- Evaluation Results ---\")\n",
    "# for result in val_results:\n",
    "#     print(f\"Q: {result['question']}\")\n",
    "#     print(f\"Generated: {result['generated_answer']}\")\n",
    "#     print(f\"Ideal: {result['ideal_answer']}\")\n",
    "#     print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
